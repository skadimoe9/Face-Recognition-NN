{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found folders: ['Azmira', 'David', 'Dimas', 'Fadhli', 'Fadlin', 'Hafidz', 'Haidar', 'Hanna', 'Keiko', 'Khansa', 'Mikhael', 'Puti', 'Raesa', 'Satwika', 'Toni']\n",
      "Input array shape: (346, 4900)\n",
      "Output array shape: (346, 15)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy.random import normal\n",
    "\n",
    "input_directory = \"../Dataset/Foto_Resize_70x70\"\n",
    "\n",
    "# List all folders in the input directory\n",
    "folders = [f for f in os.listdir(input_directory) if os.path.isdir(os.path.join(input_directory, f))]\n",
    "print(f\"Found folders: {folders}\")\n",
    "\n",
    "# Initialize arrays for inputs and outputs\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Create a mapping from folder names to one-hot encoded labels\n",
    "label_map = {folder: idx for idx, folder in enumerate(folders)}\n",
    "num_classes = len(folders)\n",
    "\n",
    "# Process each folder and photo\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(input_directory, folder)\n",
    "    \n",
    "    # Sort the list of photos to ensure consistent order\n",
    "    photos = sorted([p for p in os.listdir(folder_path) if p.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    \n",
    "    for photo in photos:\n",
    "        photo_path = os.path.join(folder_path, photo)\n",
    "        image = Image.open(photo_path)\n",
    "        \n",
    "        # Convert the image to a numpy array and flatten it\n",
    "        image_array = np.array(image).flatten()\n",
    "        X.append(image_array)\n",
    "        \n",
    "        # Create a one-hot encoded label\n",
    "        label = np.zeros(num_classes)\n",
    "        label[label_map[folder]] = 1\n",
    "        Y.append(label)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(X)\n",
    "X = X/255;\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(f\"Input array shape: {X.shape}\")\n",
    "print(f\"Output array shape: {Y.shape}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_data(X, Y, train_ratio, val_ratio):\n",
    "  # Acak data\n",
    "  num_samples = X.shape[0]  # Assume X and Y have the same number of samples\n",
    "  indices = np.arange(num_samples)\n",
    "  np.random.shuffle(indices)\n",
    "\n",
    "  # Cari banyak data atau indeks yang akan digunakan untuk training dan validasi\n",
    "  train_idx = int(train_ratio * num_samples)\n",
    "  val_idx = int((train_ratio + val_ratio) * num_samples)\n",
    "\n",
    "  # Split data\n",
    "  X_train = X[indices[:train_idx]]\n",
    "  Y_train = Y[indices[:train_idx]]\n",
    "  X_val = X[indices[train_idx:val_idx]]\n",
    "  Y_val = Y[indices[train_idx:val_idx]]\n",
    "  X_test = X[indices[val_idx:]]\n",
    "  Y_test = Y[indices[val_idx:]]\n",
    "\n",
    "  # Kembalikan X Y untuk training, validasi, dan testing\n",
    "  return X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, Y_train, X_val, Y_val, X_test, Y_test = split_data(X, Y, train_ratio=0.7, val_ratio=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "    \n",
    "def Softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "def d_ReLU(x):\n",
    "    return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 4900)\n",
      "(15, 1)\n",
      "(15, 15)\n",
      "(15, 1)\n"
     ]
    }
   ],
   "source": [
    "v = np.random.normal(loc=0, scale = stddev, size = (inputLayer, hiddenNeurons))\n",
    "v = v.T\n",
    "print(np.shape(v))\n",
    "vb = np.random.normal(loc=0, scale=stddev, size=hiddenNeurons) \n",
    "vb = vb.reshape(15,1)\n",
    "print(np.shape(vb))\n",
    "w = np.random.normal(loc=0, scale = stddev, size = (outputLayer, hiddenNeurons))\n",
    "w = w.T\n",
    "print(np.shape(w))\n",
    "wb = np.random.normal(loc=0, scale = stddev, size = outputLayer)\n",
    "wb = wb.reshape(15,1)\n",
    "print(np.shape(wb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss  0 : 340.52721678122504\n",
      "Cross-Entropy Loss  1 : 225.7475690608332\n",
      "Cross-Entropy Loss  2 : 44.66471906200502\n",
      "Cross-Entropy Loss  3 : 44.66471906200502\n",
      "Cross-Entropy Loss  4 : 44.66471906200502\n",
      "Cross-Entropy Loss  5 : 44.66471906200502\n",
      "Cross-Entropy Loss  6 : 44.66471906200502\n",
      "Cross-Entropy Loss  7 : 44.66471906200502\n",
      "Cross-Entropy Loss  8 : 44.66471906200502\n",
      "Cross-Entropy Loss  9 : 44.66471906200502\n",
      "Cross-Entropy Loss  10 : 44.66471906200502\n",
      "Cross-Entropy Loss  11 : 44.66471906200502\n",
      "Cross-Entropy Loss  12 : 44.66471906200502\n",
      "Cross-Entropy Loss  13 : 44.66471906200502\n",
      "Cross-Entropy Loss  14 : 44.66471906200502\n",
      "Cross-Entropy Loss  15 : 44.66471906200502\n",
      "Cross-Entropy Loss  16 : 44.66471906200502\n",
      "Cross-Entropy Loss  17 : 44.66471906200502\n",
      "Cross-Entropy Loss  18 : 44.66471906200502\n",
      "Cross-Entropy Loss  19 : 44.66471906200502\n",
      "Cross-Entropy Loss  20 : 44.66471906200502\n",
      "Cross-Entropy Loss  21 : 44.66471906200502\n",
      "Cross-Entropy Loss  22 : 44.66471906200502\n",
      "Cross-Entropy Loss  23 : 44.66471906200502\n",
      "Cross-Entropy Loss  24 : 44.66471906200502\n",
      "Cross-Entropy Loss  25 : 44.66471906200502\n",
      "Cross-Entropy Loss  26 : 44.66471906200502\n",
      "Cross-Entropy Loss  27 : 44.66471906200502\n",
      "Cross-Entropy Loss  28 : 44.66471906200502\n",
      "Cross-Entropy Loss  29 : 44.66471906200502\n",
      "Cross-Entropy Loss  30 : 44.66471906200502\n",
      "Cross-Entropy Loss  31 : 44.66471906200502\n",
      "Cross-Entropy Loss  32 : 44.66471906200502\n",
      "Cross-Entropy Loss  33 : 44.66471906200502\n",
      "Cross-Entropy Loss  34 : 44.66471906200502\n",
      "Cross-Entropy Loss  35 : 44.66471906200502\n",
      "Cross-Entropy Loss  36 : 44.66471906200502\n",
      "Cross-Entropy Loss  37 : 44.66471906200502\n",
      "Cross-Entropy Loss  38 : 44.66471906200502\n",
      "Cross-Entropy Loss  39 : 44.66471906200502\n",
      "Cross-Entropy Loss  40 : 44.66471906200502\n",
      "Cross-Entropy Loss  41 : 44.66471906200502\n",
      "Cross-Entropy Loss  42 : 44.66471906200502\n",
      "Cross-Entropy Loss  43 : 44.66471906200502\n",
      "Cross-Entropy Loss  44 : 44.66471906200502\n",
      "Cross-Entropy Loss  45 : 44.66471906200502\n",
      "Cross-Entropy Loss  46 : 44.66471906200502\n",
      "Cross-Entropy Loss  47 : 44.66471906200502\n",
      "Cross-Entropy Loss  48 : 44.66471906200502\n",
      "Cross-Entropy Loss  49 : 44.66471906200502\n",
      "Cross-Entropy Loss  50 : 44.66471906200502\n",
      "Cross-Entropy Loss  51 : 44.66471906200502\n",
      "Cross-Entropy Loss  52 : 44.66471906200502\n",
      "Cross-Entropy Loss  53 : 44.66471906200502\n",
      "Cross-Entropy Loss  54 : 44.66471906200502\n",
      "Cross-Entropy Loss  55 : 44.66471906200502\n",
      "Cross-Entropy Loss  56 : 44.66471906200502\n",
      "Cross-Entropy Loss  57 : 44.66471906200502\n",
      "Cross-Entropy Loss  58 : 44.66471906200502\n",
      "Cross-Entropy Loss  59 : 44.66471906200502\n",
      "Cross-Entropy Loss  60 : 44.66471906200502\n",
      "Cross-Entropy Loss  61 : 44.66471906200502\n",
      "Cross-Entropy Loss  62 : 44.66471906200502\n",
      "Cross-Entropy Loss  63 : 44.66471906200502\n",
      "Cross-Entropy Loss  64 : 44.66471906200502\n",
      "Cross-Entropy Loss  65 : 44.66471906200502\n",
      "Cross-Entropy Loss  66 : 44.66471906200502\n",
      "Cross-Entropy Loss  67 : 44.66471906200502\n",
      "Cross-Entropy Loss  68 : 44.66471906200502\n",
      "Cross-Entropy Loss  69 : 44.66471906200502\n",
      "Cross-Entropy Loss  70 : 44.66471906200502\n",
      "Cross-Entropy Loss  71 : 44.66471906200502\n",
      "Cross-Entropy Loss  72 : 44.66471906200502\n",
      "Cross-Entropy Loss  73 : 44.66471906200502\n",
      "Cross-Entropy Loss  74 : 44.66471906200502\n",
      "Cross-Entropy Loss  75 : 44.66471906200502\n",
      "Cross-Entropy Loss  76 : 44.66471906200502\n",
      "Cross-Entropy Loss  77 : 44.66471906200502\n",
      "Cross-Entropy Loss  78 : 44.66471906200502\n",
      "Cross-Entropy Loss  79 : 44.66471906200502\n",
      "Cross-Entropy Loss  80 : 44.66471906200502\n",
      "Cross-Entropy Loss  81 : 44.66471906200502\n",
      "Cross-Entropy Loss  82 : 44.66471906200502\n",
      "Cross-Entropy Loss  83 : 44.66471906200502\n",
      "Cross-Entropy Loss  84 : 44.66471906200502\n",
      "Cross-Entropy Loss  85 : 44.66471906200502\n",
      "Cross-Entropy Loss  86 : 44.66471906200502\n",
      "Cross-Entropy Loss  87 : 44.66471906200502\n",
      "Cross-Entropy Loss  88 : 44.66471906200502\n",
      "Cross-Entropy Loss  89 : 44.66471906200502\n",
      "Cross-Entropy Loss  90 : 44.66471906200502\n",
      "Cross-Entropy Loss  91 : 44.66471906200502\n",
      "Cross-Entropy Loss  92 : 44.66471906200502\n",
      "Cross-Entropy Loss  93 : 44.66471906200502\n",
      "Cross-Entropy Loss  94 : 44.66471906200502\n",
      "Cross-Entropy Loss  95 : 44.66471906200502\n",
      "Cross-Entropy Loss  96 : 44.66471906200502\n",
      "Cross-Entropy Loss  97 : 44.66471906200502\n",
      "Cross-Entropy Loss  98 : 44.66471906200502\n",
      "Cross-Entropy Loss  99 : 44.66471906200502\n"
     ]
    }
   ],
   "source": [
    "inputLayer = 4900\n",
    "hiddenLayer = 1\n",
    "outputLayer = 15\n",
    "\n",
    "X = X_train.T\n",
    "Y = Y_train.T\n",
    "\n",
    "#70x70\n",
    "#input = 4900, hiddenLayer = 1, neuron = 2\n",
    "\n",
    "n = 15\n",
    "stddev = np.sqrt(2/n)\n",
    "hiddenNeurons = 15\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    z_in = np.dot(v,X) + vb\n",
    "    z = ReLU(z_in)\n",
    "\n",
    "    y_in = np.dot(w,z) + wb\n",
    "    y = Softmax(y_in)\n",
    "\n",
    "    sigma_k = y-Y\n",
    "\n",
    "    delta_w = (1/X.shape[1])*np.dot(sigma_k,z.T)\n",
    "    delta_wb = (1/X.shape[1])*np.sum(sigma_k)\n",
    "\n",
    "    sigma_in = np.dot(w.T,sigma_k)*d_ReLU(z_in)\n",
    "\n",
    "    delta_v = (1/X.shape[1])*np.dot(sigma_in,X.T)\n",
    "    delta_vb = (1/X.shape[1])*np.sum(sigma_in)\n",
    "\n",
    "    alpha = 2\n",
    "    w -= alpha*delta_w\n",
    "    wb -= alpha*delta_wb\n",
    "    v -= alpha*delta_v\n",
    "    vb -= alpha*delta_vb\n",
    "\n",
    "    def cross_entropy_loss(y_true, y_pred):\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        # Calculate cross-entropy loss\n",
    "        loss = -np.sum(y_true * np.log(y)) / y_true.shape[0]\n",
    "        return loss\n",
    "    loss = cross_entropy_loss(Y, y)\n",
    "    print(\"Cross-Entropy Loss \",i, \":\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.8664524961266644\n",
      "Validation Accuracy: 0.057692307692307696\n"
     ]
    }
   ],
   "source": [
    "X = X_val.T\n",
    "Y = Y_val.T\n",
    "\n",
    "def validation(X, v, vb, w, wb):\n",
    "    z_in = np.dot(v,X) + vb\n",
    "    z = ReLU(z_in)\n",
    "\n",
    "    y_in = np.dot(w,z) + wb\n",
    "    y = Softmax(y_in)\n",
    "    return y\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "Y_pred = validation(X, v, vb, w, wb)\n",
    "\n",
    "# Compute loss\n",
    "loss = compute_loss(Y, Y_pred)\n",
    "print(\"Validation Loss:\", loss)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = compute_accuracy(Y, Y_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
