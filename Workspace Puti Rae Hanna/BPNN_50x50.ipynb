{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import normal\n",
    "from PIL import Image\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor:\n",
    "    def __init__(self, input_directory):\n",
    "        self.input_directory = input_directory\n",
    "        self.label_map = {}\n",
    "        self.scaler = None\n",
    "\n",
    "    def flatten_image(self):\n",
    "        # List all folders in the input directory\n",
    "        folders = [f for f in os.listdir(self.input_directory) if os.path.isdir(os.path.join(self.input_directory, f))]\n",
    "        print(f\"Found folders: {folders}\")\n",
    "\n",
    "        # Initialize arrays for inputs and outputs\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Create a mapping from folder names to one-hot encoded labels\n",
    "        self.label_map = {folder: idx for idx, folder in enumerate(folders)}\n",
    "        num_classes = len(folders)\n",
    "\n",
    "        # Process each folder and photo\n",
    "        for folder in folders:\n",
    "            folder_path = os.path.join(self.input_directory, folder)\n",
    "\n",
    "            # Sort the list of photos to ensure consistent order\n",
    "            photos = sorted([p for p in os.listdir(folder_path) if p.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "            for photo in photos:\n",
    "                photo_path = os.path.join(folder_path, photo)\n",
    "                image = Image.open(photo_path)\n",
    "\n",
    "                # Convert the image to a numpy array and flatten it\n",
    "                image_array = np.array(image).flatten()\n",
    "                X.append(image_array)\n",
    "\n",
    "                # Create a one-hot encoded label\n",
    "                label = np.zeros(num_classes)\n",
    "                label[self.label_map[folder]] = 1\n",
    "                y.append(label)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    class NormalizeImage:\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "\n",
    "        def transform(self):\n",
    "            # Normalize data to the range [-1, 1]\n",
    "            return self.data / 255\n",
    "\n",
    "        def inverse_transform(self):\n",
    "            return self.data * 255\n",
    "\n",
    "    @staticmethod\n",
    "    def split_train_test(combined_array):\n",
    "        np.random.shuffle(combined_array)\n",
    "        # Calculate the split indices\n",
    "        num_samples = combined_array.shape[0]\n",
    "        train_end = int(0.7 * num_samples)  # 70% of the data is used for training\n",
    "        test_end = int(0.85 * num_samples)  # 15% of the data is used for testing\n",
    "\n",
    "        # Split the data into training, testing, and validation sets\n",
    "        train_data = combined_array[:train_end]\n",
    "        test_data = combined_array[train_end:test_end]\n",
    "        val_data = combined_array[test_end:]\n",
    "\n",
    "        return train_data, test_data, val_data\n",
    "\n",
    "    @staticmethod\n",
    "    def split_input_output(data, num_input_features):\n",
    "        X_data = data[:, :num_input_features]\n",
    "        y_data = data[:, num_input_features:]\n",
    "        return X_data, y_data\n",
    "\n",
    "    def process_all(self):\n",
    "        X, y = self.flatten_image()\n",
    "\n",
    "        self.scaler = self.NormalizeImage(X)\n",
    "        X_normalized = self.scaler.transform()\n",
    "        combined_array = np.hstack((X_normalized, y))\n",
    "\n",
    "        train_data, test_data, val_data = self.split_train_test(combined_array)\n",
    "        num_input_features = X.shape[1]\n",
    "        X_train, y_train = self.split_input_output(train_data, num_input_features)\n",
    "        X_test, y_test = self.split_input_output(test_data, num_input_features)\n",
    "        X_val, y_val = self.split_input_output(val_data, num_input_features)\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, X_val, y_val, self.scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found folders: ['Azmira', 'David', 'Dimas', 'Fadhli', 'Fadlin', 'Hafidz', 'Haidar', 'Hanna', 'Keiko', 'Khansa', 'Mikhael', 'Puti', 'Raesa', 'Satwika', 'Toni']\n",
      "Training data: (968, 2500) (968, 15)\n",
      "Testing data: (208, 2500) (208, 15)\n",
      "Validation data: (208, 2500) (208, 15)\n",
      "Normalized training sample: [0.68627451 0.68235294 0.68235294 ... 0.70588235 0.69019608 0.69019608]\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing the dataset\n",
    "input_directory = \"../Dataset/Foto_Resize_Rotate_50x50\" \n",
    "\n",
    "# Create an instance of the ImageProcessor class\n",
    "image_processor = ImageProcessor(input_directory)\n",
    "\n",
    "# Process the data\n",
    "X_train, y_train, X_test, y_test, X_val, y_val, scaler = image_processor.process_all()\n",
    "\n",
    "# Display the shape of the datasets\n",
    "print(\"Training data:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing data:\", X_test.shape, y_test.shape)\n",
    "print(\"Validation data:\", X_val.shape, y_val.shape)\n",
    "\n",
    "# Example of normalized input\n",
    "print(\"Normalized training sample:\", X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 10)\n",
      "(1, 10)\n",
      "(10, 15)\n",
      "(1, 15)\n"
     ]
    }
   ],
   "source": [
    "#50x50\n",
    "#input = 2500, hidden layer = 1, neuron = 1680\n",
    "inputLayer = 2500\n",
    "outputLayer = 15\n",
    "epoch = 10000\n",
    "alpha = 0.01\n",
    "X = X_train\n",
    "T = y_train\n",
    "neuron = 10\n",
    "\n",
    "stddev = np.sqrt(2/inputLayer)\n",
    "\n",
    "\n",
    "v = np.random.normal(loc=0, scale = stddev, size = (inputLayer, neuron))  # weights from input to hidden layer\n",
    "print(np.shape(v))\n",
    "\n",
    "vb = np.random.normal(loc=0, scale = stddev, size = (1, neuron)) # weight biases for hidden layer\n",
    "print(np.shape(vb))\n",
    "\n",
    "w = np.random.normal(loc=0, scale = stddev, size = (neuron, outputLayer)) # weights from hidden to output layer\n",
    "print(np.shape(w))\n",
    "\n",
    "wb = np.random.normal(loc=0, scale = stddev, size = (1, outputLayer)) # weight biases for output layer\n",
    "print(np.shape(wb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Error: 1.7481\n",
      "Epoch 101: Average Error: 2.6843\n",
      "Epoch 201: Average Error: 2.3953\n",
      "Epoch 301: Average Error: 1.6995\n",
      "Epoch 401: Average Error: 1.1308\n",
      "Epoch 501: Average Error: 0.9843\n",
      "Epoch 601: Average Error: 0.8712\n",
      "Epoch 701: Average Error: 0.7914\n",
      "Epoch 801: Average Error: 0.7436\n",
      "Epoch 901: Average Error: 0.7064\n",
      "Epoch 1001: Average Error: 0.6674\n",
      "Epoch 1101: Average Error: 0.6223\n",
      "Epoch 1201: Average Error: 0.5643\n",
      "Epoch 1301: Average Error: 0.4870\n",
      "Epoch 1401: Average Error: 0.4138\n",
      "Epoch 1501: Average Error: 0.3575\n",
      "Epoch 1601: Average Error: 0.3129\n",
      "Epoch 1701: Average Error: 0.2815\n",
      "Epoch 1801: Average Error: 0.2555\n",
      "Epoch 1901: Average Error: 0.2380\n",
      "Epoch 2001: Average Error: 0.2211\n",
      "Epoch 2101: Average Error: 0.2098\n",
      "Epoch 2201: Average Error: 0.1915\n",
      "Epoch 2301: Average Error: 0.1881\n",
      "Epoch 2401: Average Error: 0.1822\n",
      "Epoch 2501: Average Error: 0.1748\n",
      "Epoch 2601: Average Error: 0.1685\n",
      "Epoch 2701: Average Error: 0.1619\n",
      "Epoch 2801: Average Error: 0.1642\n",
      "Epoch 2901: Average Error: 0.1545\n",
      "Epoch 3001: Average Error: 0.1558\n",
      "Epoch 3101: Average Error: 0.1577\n",
      "Epoch 3201: Average Error: 0.1444\n",
      "Epoch 3301: Average Error: 0.1497\n",
      "Epoch 3401: Average Error: 0.1405\n",
      "Epoch 3501: Average Error: 0.1446\n",
      "Epoch 3601: Average Error: 0.1434\n",
      "Epoch 3701: Average Error: 0.1366\n",
      "Epoch 3801: Average Error: 0.1329\n",
      "Epoch 3901: Average Error: 0.1318\n",
      "Epoch 4001: Average Error: 0.1306\n",
      "Epoch 4101: Average Error: 0.1295\n",
      "Epoch 4201: Average Error: 0.1285\n",
      "Epoch 4301: Average Error: 0.1277\n",
      "Epoch 4401: Average Error: 0.1269\n",
      "Epoch 4501: Average Error: 0.1261\n",
      "Epoch 4601: Average Error: 0.1254\n",
      "Epoch 4701: Average Error: 0.1246\n",
      "Epoch 4801: Average Error: 0.1236\n",
      "Epoch 4901: Average Error: 0.1227\n",
      "Epoch 5001: Average Error: 0.1220\n",
      "Epoch 5101: Average Error: 0.1214\n",
      "Epoch 5201: Average Error: 0.1208\n",
      "Epoch 5301: Average Error: 0.1203\n",
      "Epoch 5401: Average Error: 0.1198\n",
      "Epoch 5501: Average Error: 0.1193\n",
      "Epoch 5601: Average Error: 0.1189\n",
      "Epoch 5701: Average Error: 0.1185\n",
      "Epoch 5801: Average Error: 0.1181\n",
      "Epoch 5901: Average Error: 0.1177\n",
      "Epoch 6001: Average Error: 0.1174\n",
      "Epoch 6101: Average Error: 0.1171\n",
      "Epoch 6201: Average Error: 0.1168\n",
      "Epoch 6301: Average Error: 0.1165\n",
      "Epoch 6401: Average Error: 0.1162\n",
      "Epoch 6501: Average Error: 0.1160\n",
      "Epoch 6601: Average Error: 0.1157\n",
      "Epoch 6701: Average Error: 0.1155\n",
      "Epoch 6801: Average Error: 0.1153\n",
      "Epoch 6901: Average Error: 0.1151\n",
      "Epoch 7001: Average Error: 0.1149\n",
      "Epoch 7101: Average Error: 0.1147\n",
      "Epoch 7201: Average Error: 0.1146\n",
      "Epoch 7301: Average Error: 0.1144\n",
      "Epoch 7401: Average Error: 0.1142\n",
      "Epoch 7501: Average Error: 0.1141\n",
      "Epoch 7601: Average Error: 0.1140\n",
      "Epoch 7701: Average Error: 0.1138\n",
      "Epoch 7801: Average Error: 0.1137\n",
      "Epoch 7901: Average Error: 0.1136\n",
      "Epoch 8001: Average Error: 0.1134\n",
      "Epoch 8101: Average Error: 0.1133\n",
      "Epoch 8201: Average Error: 0.1132\n",
      "Epoch 8301: Average Error: 0.1131\n",
      "Epoch 8401: Average Error: 0.1129\n",
      "Epoch 8501: Average Error: 0.1128\n",
      "Epoch 8601: Average Error: 0.1127\n",
      "Epoch 8701: Average Error: 0.1126\n",
      "Epoch 8801: Average Error: 0.1125\n",
      "Epoch 8901: Average Error: 0.1124\n",
      "Epoch 9001: Average Error: 0.1123\n",
      "Epoch 9101: Average Error: 0.1121\n",
      "Epoch 9201: Average Error: 0.1120\n",
      "Epoch 9301: Average Error: 0.1119\n",
      "Epoch 9401: Average Error: 0.1118\n",
      "Epoch 9501: Average Error: 0.1116\n",
      "Epoch 9601: Average Error: 0.1115\n",
      "Epoch 9701: Average Error: 0.1114\n",
      "Epoch 9801: Average Error: 0.1112\n",
      "Epoch 9901: Average Error: 0.1111\n",
      "Epoch 10000: Average Error: 0.1110\n",
      "Stopping early at epoch 10000 with average error: 0.1110\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):  # Set the maximum number of epochs\n",
    "    total_error = 0  # Initialize total error for the epoch\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        # Feedforward\n",
    "        x = X[i]\n",
    "        t = T[i]  # True label for this sample\n",
    "        z_in = np.dot(x, v) + vb\n",
    "        z = 1 / (1 + np.exp(-z_in))  # Hidden layer activation\n",
    "        y_in = np.dot(z, w) + wb\n",
    "        y = 1 / (1 + np.exp(-y_in))  # Output layer activation (sigmoid)\n",
    "\n",
    "        # Flatten activations to avoid shape issues\n",
    "        z_in = z_in.flatten()\n",
    "        z = z.flatten()\n",
    "        y_in = y_in.flatten()\n",
    "        y = y.flatten()\n",
    "\n",
    "        # Backpropagation of error\n",
    "        delta_y = (t - y) * np.exp(-y_in) / ((np.exp(-y_in) + 1) ** 2)\n",
    "        delta_w = alpha * np.outer(z, delta_y)  # Outer product for weight updates\n",
    "        delta_wb = alpha * delta_y\n",
    "\n",
    "        delta_in = np.dot(delta_y, w.T)  # Backpropagated error for hidden layer\n",
    "        delta = delta_in * np.exp(-z_in) / ((np.exp(-z_in) + 1) ** 2)\n",
    "        delta_v = alpha * np.outer(x, delta)\n",
    "        delta_vb = alpha * delta\n",
    "\n",
    "        # Update weights and biases\n",
    "        w += delta_w\n",
    "        wb += delta_wb\n",
    "        v += delta_v\n",
    "        vb += delta_vb\n",
    "\n",
    "        # Calculate cross-entropy loss for this sample\n",
    "        sample_error = -np.sum(t * np.log(y + 1e-8))\n",
    "        total_error += sample_error\n",
    "\n",
    "    # Calculate average error for the epoch\n",
    "    average_error = total_error / len(X)\n",
    "\n",
    "    # Print the error every 100 epochs or the last epoch\n",
    "    if e % 100 == 0 or e == epoch - 1:\n",
    "        print(f\"Epoch {e + 1}: Average Error: {average_error:.4f}\")\n",
    "\n",
    "    # Early stopping condition\n",
    "    if average_error < 0.01 or e == epoch - 1:\n",
    "        print(f\"Stopping early at epoch {e + 1} with average error: {average_error:.4f}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
